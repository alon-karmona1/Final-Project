## ğŸ†Final Project: Dataset of drone esncountersğŸ†

**Abstract** <br>
The expansion in drones' use in recent years is a direct outcome of the research and developments made in the field. The aerodynamic abilities, lightweight and relatively low price â€“ helped the Unmanned Aerial Vehicles become a key component in different fields in the different sectors, from its use in classified military missions â€“ to a simple Pizza delivery.
Another effect of the R&D in drone technology is the level of autonomy that the drone has. The levels vary between strictly manual ones and drones that are mission-oriented when they are sent to a mission and returned upon completion.

The fast growth in their presence in populated areas has exposed another tier in the sub-field of Human-Robot Interaction (HRI) - Human Drones Interaction (HDI). That is the study of the interaction between drones with by-passers â€“ who are not the main subjects of interest in the drone's mission (not the user or the target). An autonomous drone that is sent to provide an essential drug to a patient (e.g.)  â€“ needs to know how its environment reacts to his presence and act proactively.

**In our project, we have created a dataset of annotated footage of the interaction between drones and bystanders â€“ from the point of view of the drone. After studying the different datasets of videos published in the HDI field and analyzing their specifications, we have decided to use a deception method in which we have created a situation that simulates an encounter between an autonomous drone and by-passers. The footage was collected in public areas and with people that were not aware of the experiment.**

The deception has helped us collect spontaneous reactions of participants, the participants were only notified about the experiment after the footage was already taken. This was followed by asking for their permission to use the videos for academic reasons. After granting permission, the users were also asked a few questions about their experience of the interaction. After editing and keeping the approved videos, we have done a basic Positive/Negative/Neutral annotation to the video frames. In addition, we have also pointed out the frame in which the participant notices the drone.

**The datasets will be used as the basis on which designed models will perform a process of Supervised Learning. Using this Machine Learning process, we will enable the drone to analyze its surroundings in real-time, correctly classify the nature of the interaction with by-passers and make decisions accordingly.**
<br><br><br>

**ğŸ“· Pictures from the project: ğŸ“·**
<br><br>
![Screenshot 2022-01-21 220146](https://user-images.githubusercontent.com/94222074/150592646-e72cac9b-c4d1-40c4-90a7-046e35c28417.jpg)
<br><br>
![Screenshot 2022-01-21 214657](https://user-images.githubusercontent.com/94222074/150590880-b463ed92-272e-483f-ba0f-88e000be8f8a.jpg)
<br><br><br>
![Screenshot 2022-01-21 214333](https://user-images.githubusercontent.com/94222074/150590567-84f9d0b9-b9fe-443d-ae38-fec9e7218240.jpg)

